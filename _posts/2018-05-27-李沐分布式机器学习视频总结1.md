---
layout:     post

title:      李沐分布式机器学习视频总结1

subtitle:   李沐分布式机器学习视频总结1

date:       2018-05-27

author:     Ezra

catalog: true

tags:

    - DL
---





    看李沐的分布式机器学习视频挑战：
    1. 数据量很大，单机太慢


	1. 通信带宽受限，一般比内存带宽小十倍
	2. 同步代价高， 1ms的延迟
	3. 可能出现任务故障，如何处理？（主要原因是一个任务跑得很久的时候，这个任务会被更高优先级的任务抢占资源，结果就挂了，而不是因为机器数量多而故障率高）

优化过程中的挑战：
	1. 通信带宽小
	2. 机器数量很多时，同步代价高

MPI的问题：
	1. 很难适用于稀疏问题，没有提供API来做这个问题
	2. 没有容错

Key-value store ， e.g. redis
	1. key-value 通信代价高
	2. 难以服务端编程实现


Hadoop/Spark
    BSP数据一致性问题使得实现困难，每一次执行都要同步


parameter sever 
把数据push到server，更新好的weights再pull到worker

实现上的主要贡献：
       1、牺牲一定的数据一致性来加速       
       2、容灾，机器出现故障了怎么处理

BSP, SSP ->Bounded BSP


深度学习与机器学习的不同
	1. 
任务更复杂，DL有很多层，而传统DL可能只有一层
	2. 
对计算能力的要求更高
	3. 
更容易使用编程接口




降低通信开销：
	1. 
降低通信次数，即加大batch size
	2. 
减少需要通信的参数数量, 压缩信息量
	3. 
放松数据一致性要求




MXNET相比于parameter server的改进在于增加了一层GPU作为server




















​        