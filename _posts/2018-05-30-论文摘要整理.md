

layout:     post
title:      deconvolution networks 
subtitle:   反卷积
date:       2018-06-01
author:     Ezra
catalog: true
tags:
    - Blog

Reduced Communication Techniques1,2，
	1. 1-Bit Stochastic Gradient Descent and its Application to Data-Parallel Distributed Training of Speech DNNs

                             Abstract    We show empirically that in SGD training of deep neural networks, one can, at no or nearly no loss of accuracy, quantize the gradients aggressively—to but one bit per value—if the quantization error is carried forward across minibatches (error feedback). This size reduction makes it feasible to parallelize SGD through data-parallelism with fast processors like recent GPUs.    We implement data-parallel deterministically distributed SGD by combining this finding with AdaGrad, automatic minibatch-size selection, double buffering, and model parallelism. Unexpectedly, quantization benefits AdaGrad, giving a small accuracy gain.    For a typical Switchboard DNN with 46M parameters, we reach computation speeds of 27k frames per second (kfps) when using 2880 samples per minibatch, and 51kfps with 16k, on a server with 8 K20X GPUs. This corresponds to speed-ups over a single GPU of 3.6 and 6.3, respectively. 7 training passes over 309h of data complete in under 7h. A 160M-parameter model training processes 3300h of data in under 16h on 20 dual-GPU servers—a 10 times speed-up—albeit at a small accuracy loss.


2. QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding
    In this paper, we propose Quantized SGD (QSGD), a family of compression
    schemes with convergence guarantees and good practical performance. QSGD
    allows the user to smoothly trade off communication bandwidth and convergence
    time: nodes can adjust the number of bits sent per iteration, at the cost of possibly
    higher variance. We show that this trade-off is inherent, in the sense that improving
    it past some threshold would violate information-theoretic lower bounds. QSGD
    guarantees convergence for convex and non-convex objectives, under asynchrony,
    and can be extended to stochastic variance-reduced techniques.

we examine its performance in training networks for image classification tasks (AlexNet,
Inception, ResNet, and VGG) on the ImageNet [12] and CIFAR-10 [25] datasets, as well as on
LSTMs [19] for speech recognition. We implement QSGD in Microsoft CNTK [3].


3.Communication Quantization for Data-parallel Training of Deep Neural Networks

We study data-parallel training of deep neural networks on high-performance computing infrastructure. The key problem with scaling data-parallel training is avoiding severe communication/computation imbalance. We explore
quantizing gradient updates before communication to reduce
bandwidth requirements and compare it against a baseline
implementation that uses the MPI allreduce routine. We port
two existing quantization approaches, one-bit and threshold,
and develop our own adaptive quantization algorithm. The
performance of these algorithms is evaluated and compared
with MPI_Allreduce when training models for the MNIST
dataset and on a synthetic benchmark. On an HPC system,
MPI_Allreduce outperforms the existing quantization approaches. Our adaptive quantization is comparable or superior
for large layers without sacrificing accuracy. It is 1.76 times
faster than the next best approach for the largest layers in our
benchmark and achieves near-linear speedup in data-parallel
training.

4. Sparse Communication for Distributed Gradient Descent
    We make distributed stochastic gradient
    descent faster by exchanging sparse updates instead of dense updates. Gradient updates are positively skewed as most
    updates are near zero, so we map the
    99% smallest updates (by absolute value)
    to zero then exchange sparse matrices.
    This method can be combined with quantization to further improve the compression. We explore different configurations and apply them to neural machine
    translation and MNIST image classification tasks. Most configurations work on
    MNIST, whereas different configurations
    reduce convergence rate on the more complex translation task. Our experiments
    show that we can achieve up to 49% speed
    up on MNIST and 22% on NMT without
    damaging the final accuracy or BLEU

4.Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training

Large-scale distributed training requires significant communication bandwidth for
gradient exchange. The intensive gradient communication limits the scalability of
multi-node training, and requires expensive high-bandwidth network infrastructure.
The situation gets even worse with distributed training on mobile devices (federated
learning), which suffers from higher latency, lower throughput, intermittent poor
connections. In this paper, we propose Deep Gradient Compression that can reduce
the communication bandwidth by two orders of magnitude. We proposed four
techniques that also preserves the accuracy: momentum correction, local gradient
clipping, momentum factor masking, and warm-up training. We extensively applied
Deep Gradient Compression to many types of machine learning tasks including
image classification, speech recognition, and language modeling with multiple
datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On
these scenarios, Deep Gradient Compression achieved a compression ratio from
270× to 600× without losing accuracy, cutting the gradient size of ResNet-50
from 97MB to 0.35MB, DeepSpeech from 488MB to 0.74MB. Deep gradient
compression enables large-scale distributed training on inexpensive commodity
1Gbps Ethernet and facilitates distributed training on mobile.

5.meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting
We propose a simple yet effective technique
for neural network learning. The forward
propagation is computed as usual. In back propagation, only a small subset of the full gradient
is computed to update the model parameters.
The gradient vectors are sparsified in such a
way that only the top-k elements (in terms of
magnitude) are kept. As a result, only k rows or
columns (depending on the layout) of the weight
matrix are modified, leading to a linear reduction
(k divided by the vector dimension) in the
computational cost. Surprisingly, experimental
results demonstrate that we can update only
1–4% of the weights at each back propagation
pass. This does not result in a larger number
of training iterations. More interestingly, the
accuracy of the resulting models is actually
improved rather than degraded, and a detailed analysis is given. The code is available at
https://github.com/jklj077/meProp



Lossless Methods

1.Project Adam: Building an Efficient and Scalable Deep Learning Training System

![img](https://github.com/ezraxe/ezraxe.github.io/raw/master/img/Image(25).png)  2.Petuum: A New Platform for Distributed Machine Learning on Big Data ![img](file:///C:/Users/Ezra/AppData/Local/Temp/enhtmlclip/Image(26).png) 

3. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour![img](https://github.com/ezraxe/ezraxe.github.io/raw/master/img/Image(27).png) 1.S-Caffe: Co-designing MPI Runtimes and Caffe for Scalable Deep Learning on Modern GPU Clusters![img](https://github.com/ezraxe/ezraxe.github.io/raw/master/img/Image(28).png) Sparse Reduction1. Mpi reduction operations for sparse floating-point data![img](https://github.com/ezraxe/ezraxe.github.io/raw/master/img/Image(29).png)2.  Kylix: A Sparse Allreduce for Commodity Clusters ![img](https://github.com/ezraxe/ezraxe.github.io/raw/master/img/Image(30).png) 3. Communication quantization for data-parallel training of deep neural networks ![img](https://github.com/ezraxe/ezraxe.github.io/raw/master/img/Image(31).png)           